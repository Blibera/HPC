from hmmlearn.hmm import MultinomialHMM
from hmmlearn import hmm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys

Full_data = pd.read_csv("C:/Users/DI_Lab/Desktop/20년도 Kisti 과제/HMM/hmm learn/원본 자료/edit2_ver_PCA_variable.csv")
Full_data = Full_data.dropna()
x = Full_data['state']
bob_saysten = x.head(10)
x = np.atleast_2d(x)
bob_says = x.T

bob_saysten = [[112],[0],[127]]
print(bob_saysten)
y = Full_data['total']
states = ['High_Down', 'Low_Down', 'Low_Up', 'High_Up']
n_states = len(states)

observations = [i for i in range(128)]
n_observations = len(observations)

model = hmm.MultinomialHMM(n_components=n_states, init_params="ste", verbose='True', n_iter=100)
model.startprob_ = np.array([0.403, 0.366, 0.117, 0.114])
model.transmat_ = np.array([
    [0.028, 0.228, 0.404, 0.34 ],
    [0.021, 0.246, 0.616, 0.117],
    [0.052, 0.569, 0.304, 0.075],
    [0.744, 0.175, 0.07 , 0.011]
])

model.emissionprob_ = np.array([[0.856, 0.106, 0.   , 0.016, 0.   , 0.001, 0.   , 0.011, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.005, 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   ], [0.37 , 0.08 , 0.01 , 0.023, 0.008, 0.006, 0.015, 0.04 , 0.002,
        0.001, 0.001, 0.001, 0.001, 0.001, 0.012, 0.022, 0.001, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.008, 0.005, 0.001,
        0.002, 0.003, 0.002, 0.016, 0.066, 0.008, 0.003, 0.001, 0.001,
        0.002, 0.001, 0.002, 0.003, 0.   , 0.001, 0.   , 0.   , 0.   ,
        0.   , 0.001, 0.001, 0.002, 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.003, 0.001, 0.001, 0.   , 0.002, 0.001, 0.004,
        0.013, 0.038, 0.007, 0.003, 0.003, 0.005, 0.002, 0.008, 0.01 ,
        0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.002, 0.003, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.001,
        0.001, 0.   , 0.002, 0.   , 0.013, 0.017, 0.024, 0.001, 0.002,
        0.002, 0.002, 0.002, 0.006, 0.007, 0.   , 0.   , 0.   , 0.   ,
        0.001, 0.   , 0.   , 0.002, 0.004, 0.001, 0.   , 0.   , 0.   ,
        0.   , 0.001, 0.   , 0.005, 0.003, 0.001, 0.003, 0.001, 0.002,
        0.016, 0.046], [0.03 , 0.005, 0.001, 0.001, 0.001, 0.001, 0.003, 0.003, 0.   ,
        0.001, 0.   , 0.001, 0.   , 0.   , 0.002, 0.001, 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.001,
        0.   , 0.   , 0.   , 0.003, 0.005, 0.014, 0.002, 0.002, 0.001,
        0.002, 0.001, 0.002, 0.004, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.001, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.002, 0.001, 0.001, 0.002, 0.001, 0.001, 0.005,
        0.021, 0.013, 0.002, 0.002, 0.001, 0.001, 0.001, 0.008, 0.003,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.002, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,
        0.   , 0.   , 0.001, 0.   , 0.014, 0.007, 0.091, 0.004, 0.005,
        0.003, 0.007, 0.004, 0.014, 0.022, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.001, 0.005, 0.042, 0.001, 0.002, 0.002, 0.003,
        0.001, 0.005, 0.005, 0.032, 0.003, 0.006, 0.007, 0.02 , 0.006,
        0.118, 0.408], [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.001, 0.001, 0.   , 0.   , 0.   , 0.001, 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.004, 0.002, 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.001, 0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,
        0.104, 0.883]])

model = model.fit(bob_says)
bob_says = np.array([[0, 2, 1, 1, 2, 0]]).T
logprob, alice_hears = model.decode(bob_says, algorithm='viterbi')
print("Bob says:", ", ".join(map(lambda x: observations[x], bob_says)))
print("Alice hears:", ", ".join(map(lambda x: states[x], alice_hears)))

print(model)
print(alice_hears)
print(logprob)